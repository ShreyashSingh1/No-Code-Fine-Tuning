{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to load model and tokenizer\n",
    "def load_model_and_tokenizer(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to load data\n",
    "def load_data(file, file_format=\"csv\"):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, mode=\"wb\") as tmp_file:\n",
    "        tmp_file.write(file.getvalue())  \n",
    "        tmp_file_path = tmp_file.name\n",
    "    \n",
    "    if file_format == \"csv\":\n",
    "        dataset = load_dataset(\"csv\", data_files={\"train\": tmp_file_path})\n",
    "    elif file_format == \"json\":\n",
    "        dataset = load_dataset(\"json\", data_files={\"train\": tmp_file_path})\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use 'csv' or 'json'.\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Function to format data\n",
    "def format_data(dataset, tokenizer, input_column):\n",
    "    def preprocess_function(examples):\n",
    "        return tokenizer(examples[input_column], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    dataset = dataset.map(preprocess_function, batched=True)\n",
    "    return dataset\n",
    "\n",
    "# Function to perform fine-tuning\n",
    "def fine_tune(model, tokenizer, dataset, param_grid, output_dir):\n",
    "    best_loss = float(\"inf\")\n",
    "    best_params = None\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    for idx, params in enumerate(grid):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=os.path.join(output_dir, f\"run_{idx}\"),\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            per_device_train_batch_size=params[\"batch_size\"],\n",
    "            num_train_epochs=params[\"epochs\"],\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=10,\n",
    "            save_total_limit=1\n",
    "        )\n",
    "        \n",
    "        # Remove `processing_class` and directly handle tokenization within the dataset\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"train\"].select(range(100)),  # Use a small test set\n",
    "        )\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        if eval_results[\"eval_loss\"] < best_loss:\n",
    "            best_loss = eval_results[\"eval_loss\"]\n",
    "            best_params = params\n",
    "            best_model_dir = os.path.join(output_dir, f\"run_{idx}\")\n",
    "    \n",
    "    return best_params, best_loss, best_model_dir\n",
    "\n",
    "# Function to compress and create a zip file\n",
    "def create_zip_from_dir(output_dir, zip_name=\"fine_tuned_model.zip\"):\n",
    "    zip_path = os.path.join(output_dir, zip_name)\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(output_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, output_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "    return zip_path\n",
    "\n",
    "# Streamlit UI\n",
    "def main():\n",
    "    st.title(\"Agentic AI Fine-Tuning Tool\")\n",
    "    st.write(\"Fine-tune Hugging Face models in 4 simple steps!\")\n",
    "\n",
    "    # Step 1: Model ID input\n",
    "    st.header(\"Step 1: Provide Hugging Face Model ID\")\n",
    "    model_id = st.text_input(\"Enter the Hugging Face model ID (e.g., bert-base-uncased):\", \"bert-base-uncased\")\n",
    "\n",
    "    # Step 2: Upload dataset and handle it directly here\n",
    "    st.header(\"Step 2: Upload Your Dataset\")\n",
    "    uploaded_file = st.file_uploader(\"Upload your dataset (CSV or JSON format):\", type=[\"csv\", \"json\"])\n",
    "    file_format = st.selectbox(\"Select the file format of your dataset:\", [\"csv\", \"json\"])\n",
    "    input_column = st.text_input(\"Enter the column name containing text input:\", \"text\")\n",
    "\n",
    "    # Check if the user uploaded a file\n",
    "    if uploaded_file is not None:\n",
    "        # Handle CSV file upload\n",
    "        if file_format == \"csv\":\n",
    "            # Load the dataset into pandas DataFrame\n",
    "            df = pd.read_csv(uploaded_file)\n",
    "            st.write(\"CSV File Data:\")\n",
    "            st.dataframe(df)\n",
    "            if input_column in df.columns:\n",
    "                st.write(f\"Displaying text data from the '{input_column}' column:\")\n",
    "                st.write(df[input_column].head())\n",
    "            else:\n",
    "                st.error(f\"Column '{input_column}' not found in the dataset.\")\n",
    "        \n",
    "        # Handle JSON file upload\n",
    "        elif file_format == \"json\":\n",
    "            try:\n",
    "                json_data = json.loads(uploaded_file.getvalue())\n",
    "                st.write(\"JSON File Data:\")\n",
    "                st.json(json_data)\n",
    "                # Check if input_column exists in the first JSON object\n",
    "                if isinstance(json_data, list) and len(json_data) > 0 and input_column in json_data[0]:\n",
    "                    st.write(f\"Displaying text data from the '{input_column}' field:\")\n",
    "                    st.write([item[input_column] for item in json_data[:5]])  # Show first 5 entries\n",
    "                else:\n",
    "                    st.error(f\"Field '{input_column}' not found in the JSON data.\")\n",
    "            except json.JSONDecodeError:\n",
    "                st.error(\"Failed to decode JSON content.\")\n",
    "\n",
    "    # Step 3: Parameter Grid for fine-tuning\n",
    "    st.header(\"Step 3: Define Hyperparameters\")\n",
    "    learning_rates = st.text_input(\"Learning rates (comma-separated):\", \"5e-5,3e-5\")\n",
    "    batch_sizes = st.text_input(\"Batch sizes (comma-separated):\", \"16,32\")\n",
    "    epochs = st.text_input(\"Epochs (comma-separated):\", \"2,3\")\n",
    "\n",
    "    # Step 4: Start Fine-Tuning\n",
    "    st.header(\"Step 4: Start Fine-Tuning\")\n",
    "    output_dir = st.text_input(\"Enter the output directory to save results:\", \"./results\")\n",
    "    start_button = st.button(\"Start Fine-Tuning\")\n",
    "\n",
    "    # When Start Button is clicked\n",
    "    if start_button and uploaded_file:\n",
    "        st.write(\"Loading model and tokenizer...\")\n",
    "        model, tokenizer = load_model_and_tokenizer(model_id)\n",
    "\n",
    "        st.write(\"Loading dataset...\")\n",
    "        dataset = load_data(uploaded_file, file_format)\n",
    "        dataset = format_data(dataset[\"train\"], tokenizer, input_column)\n",
    "        dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "        st.write(\"Starting fine-tuning...\")\n",
    "        param_grid = {\n",
    "            \"learning_rate\": [float(x) for x in learning_rates.split(\",\")],\n",
    "            \"batch_size\": [int(x) for x in batch_sizes.split(\",\")],\n",
    "            \"epochs\": [int(x) for x in epochs.split(\",\")]\n",
    "        }\n",
    "\n",
    "        # Display parameters nicely\n",
    "        with st.expander(\"Hyperparameters for Fine-Tuning\"):\n",
    "            param_df = pd.DataFrame(param_grid)\n",
    "            st.dataframe(param_df)\n",
    "\n",
    "        best_params, best_loss, best_model_dir = fine_tune(model, tokenizer, dataset, param_grid, output_dir)\n",
    "\n",
    "        st.success(f\"Fine-tuning completed! Best parameters: {best_params} with loss: {best_loss}\")\n",
    "\n",
    "        # Compress and provide download link\n",
    "        st.write(\"Preparing model for download...\")\n",
    "\n",
    "        # Show results with better styling\n",
    "        with st.expander(\"Fine-Tuning Results\"):\n",
    "            st.markdown(f\"### Best Parameters:\\n- **Learning Rate**: {best_params['learning_rate']}\\n- **Batch Size**: {best_params['batch_size']}\\n- **Epochs**: {best_params['epochs']}\")\n",
    "            st.markdown(f\"### Best Loss: **{best_loss}**\")\n",
    "\n",
    "        zip_path = create_zip_from_dir(best_model_dir)\n",
    "        with open(zip_path, \"rb\") as file:\n",
    "            st.download_button(\n",
    "                label=\"Download Fine-Tuned Model\",\n",
    "                data=file,\n",
    "                file_name=\"fine_tuned_model.zip\",\n",
    "                mime=\"application/zip\"\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from src.pipeline.agents import ClassificationAgentTunner\n",
    "\n",
    "def main():\n",
    "    st.title(\"Agentic AI Fine-Tuning Tool\")\n",
    "    st.write(\"Fine-tune Hugging Face models in 4 simple steps!\")\n",
    "\n",
    "    # Step 1: Model ID input\n",
    "    st.header(\"Step 1: Provide Hugging Face Model ID\")\n",
    "    model_id = st.text_input(\"Enter the Hugging Face model ID (e.g., albert-base-v2):\", \"albert-base-v2\")\n",
    "\n",
    "    # Step 2: Upload dataset and handle it directly here\n",
    "    st.header(\"Step 2: Upload Your Dataset\")\n",
    "    uploaded_file = st.file_uploader(\"Upload your dataset (CSV or JSON format):\", type=[\"csv\", \"json\"])\n",
    "    file_format = st.selectbox(\"Select the file format of your dataset:\", [\"csv\", \"json\"])\n",
    "    input_column = st.text_input(\"Enter the column name containing text input:\", \"text\")\n",
    "\n",
    "    # Step 3: Parameter Grid for fine-tuning\n",
    "    st.header(\"Step 3: Define Hyperparameters\")\n",
    "    learning_rates = st.text_input(\"Learning rates (comma-separated):\", \"5e-5,3e-5\")\n",
    "    batch_sizes = st.text_input(\"Batch sizes (comma-separated):\", \"16,32\")\n",
    "    epochs = st.text_input(\"Epochs (comma-separated):\", \"2,3\")\n",
    "\n",
    "    # Step 4: Start Fine-Tuning\n",
    "    st.header(\"Step 4: Start Fine-Tuning\")\n",
    "    output_dir = st.text_input(\"Enter the output directory to save results:\", \"./results\")\n",
    "    start_button = st.button(\"Start Fine-Tuning\")\n",
    "\n",
    "    # When Start Button is clicked\n",
    "    if start_button and uploaded_file:\n",
    "        st.write(\"Initializing FineTuner...\")\n",
    "        fine_tuner = ClassificationAgent(model_id, output_dir)\n",
    "\n",
    "        st.write(\"Loading dataset...\")\n",
    "        dataset = fine_tuner.load_data(uploaded_file, file_format)\n",
    "        dataset = fine_tuner.format_data(dataset[\"train\"], input_column)\n",
    "        dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "        st.write(\"Starting fine-tuning...\")\n",
    "        param_grid = {\n",
    "            \"learning_rate\": [float(x) for x in learning_rates.split(\",\")],\n",
    "            \"batch_size\": [int(x) for x in batch_sizes.split(\",\")],\n",
    "            \"epochs\": [int(x) for x in epochs.split(\",\")]\n",
    "        }\n",
    "\n",
    "        # Display parameters nicely\n",
    "        with st.expander(\"Hyperparameters for Fine-Tuning\"):\n",
    "            param_df = pd.DataFrame(param_grid)\n",
    "            st.dataframe(param_df)\n",
    "\n",
    "        best_params, best_loss, best_model_dir = fine_tuner.fine_tune(dataset, param_grid)\n",
    "\n",
    "        st.success(f\"Fine-tuning completed! Best parameters: {best_params} with loss: {best_loss}\")\n",
    "\n",
    "        # Compress and provide download link\n",
    "        st.write(\"Preparing model for download...\")\n",
    "\n",
    "        # Show results with better styling\n",
    "        with st.expander(\"Fine-Tuning Results\"):\n",
    "            st.markdown(f\"### Best Parameters:\\n- **Learning Rate**: {best_params['learning_rate']}\\n- **Batch Size**: {best_params['batch_size']}\\n- **Epochs**: {best_params['epochs']}\")\n",
    "            st.markdown(f\"### Best Loss: **{best_loss}**\")\n",
    "\n",
    "        zip_path = fine_tuner.create_zip_from_dir(best_model_dir)\n",
    "        with open(zip_path, \"rb\") as file:\n",
    "            st.download_button(\n",
    "                label=\"Download Fine-Tuned Model\",\n",
    "                data=file,\n",
    "                file_name=\"fine_tuned_model.zip\",\n",
    "                mime=\"application/zip\"\n",
    "            )\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a946fdc038c470aacc4398ea5d2aee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aedf6e50cf9475a8a79bcb9276fbf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f7fd1e45f7416c959cd5a34f81c960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48774309e2a44d9e9c7f9cfe6eb1fb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/157M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556bffc08638405bb4b7de5d4ad0a1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a974d37a31d4b538dd7bb344d94f980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4f81e0cde3455a9698548f64bf90f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8421de4736846ea8324ba88daad4b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load a dataset (e.g., Wikitext)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split='train')\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"wikitext_103.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098ff48ba34d449a97c04d0b10f21999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f0349d0a434405b0957e2f5aa788dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63b14cb44ac44c6929740c060585130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160e446c66b244b1a0db4a32954e3cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1fdec599344a8b932775fb82b15aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5e01b777e746ba89a9c9d9c6c30b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 10.58 MB\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the WikiText-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({\"text\": dataset[\"text\"]})\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"wikitext2_test.csv\", index=False)\n",
    "\n",
    "# Check the file size\n",
    "import os\n",
    "print(f\"File size: {os.path.getsize('wikitext2_test.csv') / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\shrey\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class LLMAgentTunner:\n",
    "    def __init__(self, model_id, output_dir):\n",
    "        self.model_id = model_id\n",
    "        self.output_dir = output_dir\n",
    "        self.model, self.tokenizer = self.load_model_and_tokenizer()\n",
    "\n",
    "    def load_model_and_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        model = AutoModelForCausalLM.from_pretrained(self.model_id)\n",
    "        return model, tokenizer\n",
    "\n",
    "    def load_data(self, file, file_format=\"csv\"):\n",
    "        \"\"\"Load dataset from file (CSV or JSON).\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode=\"wb\") as tmp_file:\n",
    "            tmp_file.write(file.getvalue())\n",
    "            tmp_file_path = tmp_file.name\n",
    "\n",
    "        if file_format == \"csv\":\n",
    "            dataset = load_dataset(\"csv\", data_files={\"train\": tmp_file_path})\n",
    "        elif file_format == \"json\":\n",
    "            dataset = load_dataset(\"json\", data_files={\"train\": tmp_file_path})\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Use 'csv' or 'json'.\")\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def format_data(self, dataset, input_column, target_column=None):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset for text generation.\n",
    "        If `target_column` is provided, format it for text-to-text tasks.\n",
    "        \"\"\"\n",
    "        def preprocess_function(examples):\n",
    "            if target_column:\n",
    "                # Text-to-text tasks: input -> output\n",
    "                return self.tokenizer(\n",
    "                    examples[input_column],\n",
    "                    text_target=examples[target_column],\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=128,\n",
    "                )\n",
    "            else:\n",
    "                # Text generation tasks: input only\n",
    "                return self.tokenizer(\n",
    "                    examples[input_column],\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    max_length=128,\n",
    "                )\n",
    "        dataset = dataset.map(preprocess_function, batched=True)\n",
    "        return dataset\n",
    "\n",
    "    def fine_tune(self, dataset, param_grid):\n",
    "        \"\"\"Fine-tune the model using hyperparameter search.\"\"\"\n",
    "        best_loss = float(\"inf\")\n",
    "        best_params = None\n",
    "        grid = list(ParameterGrid(param_grid))\n",
    "        for idx, params in enumerate(grid):\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=os.path.join(self.output_dir, f\"run_{idx}\"),\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"epoch\",\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                per_device_train_batch_size=params[\"batch_size\"],\n",
    "                num_train_epochs=params[\"epochs\"],\n",
    "                logging_dir=\"./logs\",\n",
    "                logging_steps=10,\n",
    "                save_total_limit=1\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset[\"train\"],\n",
    "                eval_dataset=dataset[\"train\"].select(range(100)),  # For quicker evaluation\n",
    "            )\n",
    "            trainer.train()\n",
    "            eval_results = trainer.evaluate()\n",
    "\n",
    "            if eval_results[\"eval_loss\"] < best_loss:\n",
    "                best_loss = eval_results[\"eval_loss\"]\n",
    "                best_params = params\n",
    "                best_model_dir = os.path.join(self.output_dir, f\"run_{idx}\")\n",
    "\n",
    "        return best_params, best_loss, best_model_dir\n",
    "\n",
    "    def generate_text(self, prompt, max_length=50):\n",
    "        \"\"\"Generate text based on a prompt.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(**inputs, max_length=max_length)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def create_zip_from_dir(self, best_model_dir, zip_name=\"fine_tuned_model.zip\"):\n",
    "        \"\"\"Zip the best model directory.\"\"\"\n",
    "        zip_path = os.path.join(self.output_dir, zip_name)\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, _, files in os.walk(best_model_dir):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, best_model_dir)\n",
    "                    zipf.write(file_path, arcname)\n",
    "        return zip_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "\n",
    "def main():\n",
    "    st.title(\"LLM Fine-Tuning Tool\")\n",
    "    st.write(\"Fine-tune Hugging Face models in 4 simple steps!\")\n",
    "\n",
    "    # Step 1: Model ID input\n",
    "    st.header(\"Step 1: Provide Hugging Face Model ID\")\n",
    "    model_id = st.text_input(\"Enter the Hugging Face model ID (e.g., gpt2):\", \"gpt2\")\n",
    "\n",
    "    # Step 2: Upload dataset and handle it directly here\n",
    "    st.header(\"Step 2: Upload Your Dataset\")\n",
    "    uploaded_file = st.file_uploader(\"Upload your dataset (CSV or JSON format):\", type=[\"csv\", \"json\"])\n",
    "    file_format = st.selectbox(\"Select the file format of your dataset:\", [\"csv\", \"json\"])\n",
    "    input_column = st.text_input(\"Enter the column name containing text input:\", \"text\")\n",
    "\n",
    "    # Step 3: Parameter Grid for fine-tuning\n",
    "    st.header(\"Step 3: Define Hyperparameters\")\n",
    "    learning_rates = st.text_input(\"Learning rates (comma-separated):\", \"5e-5,3e-5\")\n",
    "    batch_sizes = st.text_input(\"Batch sizes (comma-separated):\", \"16,32\")\n",
    "    epochs = st.text_input(\"Epochs (comma-separated):\", \"2,3\")\n",
    "\n",
    "    # Step 4: Start Fine-Tuning\n",
    "    st.header(\"Step 4: Start Fine-Tuning\")\n",
    "    output_dir = st.text_input(\"Enter the output directory to save results:\", \"./results\")\n",
    "    start_button = st.button(\"Start Fine-Tuning\")\n",
    "\n",
    "    # When Start Button is clicked\n",
    "    if start_button and uploaded_file:\n",
    "        st.write(\"Initializing FineTuner...\")\n",
    "        fine_tuner = LLMAgentTunner(model_id, output_dir)\n",
    "\n",
    "        st.write(\"Loading dataset...\")\n",
    "        dataset = fine_tuner.load_data(uploaded_file, file_format)\n",
    "        dataset = fine_tuner.format_data(dataset[\"train\"], input_column)\n",
    "        dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "        st.write(\"Starting fine-tuning...\")\n",
    "        param_grid = {\n",
    "            \"learning_rate\": [float(x) for x in learning_rates.split(\",\")],\n",
    "            \"batch_size\": [int(x) for x in batch_sizes.split(\",\")],\n",
    "            \"epochs\": [int(x) for x in epochs.split(\",\")]\n",
    "        }\n",
    "\n",
    "        # Display parameters nicely\n",
    "        with st.expander(\"Hyperparameters for Fine-Tuning\"):\n",
    "            param_df = pd.DataFrame(param_grid)\n",
    "            st.dataframe(param_df)\n",
    "\n",
    "        best_params, best_loss, best_model_dir = fine_tuner.fine_tune(dataset, param_grid)\n",
    "\n",
    "        st.success(f\"Fine-tuning completed! Best parameters: {best_params} with loss: {best_loss}\")\n",
    "\n",
    "        # Compress and provide download link\n",
    "        st.write(\"Preparing model for download...\")\n",
    "\n",
    "        # Show results with better styling\n",
    "        with st.expander(\"Fine-Tuning Results\"):\n",
    "            st.markdown(f\"### Best Parameters:\\n- **Learning Rate**: {best_params['learning_rate']}\\n- **Batch Size**: {best_params['batch_size']}\\n- **Epochs**: {best_params['epochs']}\")\n",
    "            st.markdown(f\"### Best Loss: **{best_loss}**\")\n",
    "\n",
    "        zip_path = fine_tuner.create_zip_from_dir(best_model_dir)\n",
    "        with open(zip_path, \"rb\") as file:\n",
    "            st.download_button(\n",
    "                label=\"Download Fine-Tuned Model\",\n",
    "                data=file,\n",
    "                file_name=\"fine_tuned_model.zip\",\n",
    "                mime=\"application/zip\"\n",
    "            )\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
